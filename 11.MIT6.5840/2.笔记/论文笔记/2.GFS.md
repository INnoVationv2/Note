# GFS

# 1.Introduction

基于谷歌内部使用提出的需求设计而出，和传统分布式存储有很大差异:

- 节点down机不看做异常，而是运行必然会出现的事情，因此集群从一开始就必须设计好监视和错误检测，容错，自动恢复功能。
- 设计好IO，如数据块的大小，管理几十亿个KB级别的数据很麻烦
- 大多数文件的操作是追加，因此追加的性能很重要

- 增加灵活性，把对GFS的操作和文件系统的API进行统一，比如可以多个client并发追加内容到一个文件，无需额外的同步操作

# 2.Design Overview

## 2.1 Assumption

- 整个系统使用商品级机器，状态监测、容错，及时恢复都必须做到
- 两种读取的工作负载最常见
  - 大型流读取： 几百KB甚至1mb，读取连续区域
  - 小型随机读取：几KB，堆积读取，编写良好的程序会把多次读取综合起来，然后以顺序方式进行读取，而不是来回寻找
- 文件追加负载：大小和读取相似，一旦写入，很少修改，必须是指随机读写，但是效率不需要很高
- 必须良好支持并发追加，提供原子性
- 要求高带宽，不注重低延迟

## 2.2 Interface

- 提供类似于POSIX标准API的文件操作接口
- 具有快照和记录追加功能
  - 快照能够以较低的开销快速创建一个文件或目录树的副本
  - 记录追加允许多个Client同时向同一个文件进行并发追加，同时保证追加的原子性

## 2.3 Architecture

<img src="http://pic-save-fury.oss-cn-shanghai.aliyuncs.com/uPic/image-20230328160900140.png" alt="image-20230328160900140" style="zoom:40%;" align="left"/>

- 一个Master，多个chunkserver，运行在商品级Linux服务器上
- 每一个文件都被分割成固定大小的`chunk`，每一个chunk都有一个64位标识，由master分配
- chunk存储在Linux服务器上，通过标识和offset进行读写
- 每一个chunk文件都会在多个server上创建副本，默认是3个副本，可以对不同级别的文件设置不同的副本数量
- master保存所有文件的信息：地址，访问权限，文件到chunk的映射，chunk的地址等。还控制着系统级别的活动，比如孤儿块的垃圾处理，chunk在chunkserver之间的迁移，master会定期和chunkserver进行心跳检测，并通过指令收集他们的状态
- 应用通过GFS Client访问GFS，Client和master进行元数据访问，和chunk服务器进行文件数据访问，不提供POSIX API，所以不需要hook到Linux的vnode层
- client和chunk server都不提供缓存服务，因为数据量很大很难cache，取消cache还可以免除缓存一致性的问题，但是Client会缓存metadata的数据；其次chunkserver运行在linux服务器上，linux文件系统本身就会将经常访问的数据保存在内存中。

## 2.4 Single master

- 简化设计，master能掌握全局信息，进行精确的`chunk`、副本放置
- 为降低Master的负载，Client通过Master获取目标文件的地址，然后将地址信息存储下来，直接向`chunk server`发送请求。
- Client会将多个块请求合并到一起以提高效率

## 2.5 Chunk Size

- 64MB，使用懒分配减少内部碎片和空间浪费
- 大Chunk的好处：
  1. 减少了client和master的交互次数，一次读取可能就取出所有需要的文件
  2. 单个Chunk大，那么总的Chunk就少，这样Client可以cache更多Chunk信息
  3. Chunk大，client从单个Chunk就能读出所有需要的数据，减少开销
  4. 减少了master存储的信息的个数，允许master将所有元信息存储在内存中
- 坏处
  - Chunk大，可能造成一个文件整个都存在一个Chunk中，当多个client对这个文件进行读写，就会出现热点问题，解决方案：
    - 对这个文件设置更高的复制因子
    - 让多个应用程序错开访问
  - 一个可能的长期解决方法：P2P，`client`从其他已读取的`client`获取文件

## 2.6 Metadata

1. Master存储三类数据在内存：`file`和`chunk`的命名空间、从文件到`chunk`的映射、`chunk`副本的位置，
2. 前两个数据也会通过`log`持久化在本地磁盘上，并存一个副本在别的服务器，使用`log`可以保证崩溃时的一致性
3. `Master`不会持久化保存每一个`chunk`的位置信息，他会在`chunk server`加入集群时询问`chunk`的信息

### 1. In-Memory Data Structures

- `metadata`都存储在`master`的内存中，所以`master`的操作很快
- `master`可以在后台周期性地扫描维护在内存中的集群状态信息，以实现gc、`chunk server`故障时的复制、以及为实现负载均衡和提高硬盘空间利用率的块迁移。
- 唯一的问题是内存大小会限制chunk数量，但是能花钱买来的都不是事

### 2.Chunk Locations

- `Master`不会持久保存`Chunk Server`的存储信息在磁盘上，比如`Chunk Server`有哪些`chunk`，
- `Master`只会在启动时从`Chunk Server`获取这些信息并在之后通过监测和心跳信息保持数据一致性

### 3. Operation Log

- `Operation Log`记录了`Meta Data`的所有操作，持久化保存
- 文件和块，以及它们的版本(参见4.5节)，都是由创建时间唯一且永久地标识的。

- 在`Log`持久化之前，`metadata`的更改不能对`Client`可见
- 在多个服务器上进行副本存储并持续更新，每次更新，只有当本地和副本`log`都成功刷入磁盘才响应客户端操作
- Master会讲多个日志合并处理，提高吞吐量
- Master通过re-do log恢复之前的状态
- `Master`会周期性创建检查点，失败后只需载入检查点并重放`log`即可恢复
- 旧的检查点可以删除，也可以保留下来以防万一
- 检查点维护期间出错不会影响使用，恢复代码会检查并跳过错误的检查点
- 为保证创建检查点不会导致master停止服务，采用的方法是在一个单独的线程中创建检查点，同时`Master`切换到一个新的日志文件记录
- 检查点和日志都会写入到本地和远程磁盘。

# 3 Consistency Model

## 1.Guarantees by GFS

<img src="http://pic-save-fury.oss-cn-shanghai.aliyuncs.com/uPic/image-20230329014148155.png" alt="image-20230329014148155" style="zoom:40%;" align="left"/>

- 文件命名空间的更改（例如创建文件）具有原子性，由Master进行控制，`namespace locking`保证了原子性和正确性，`master log`的记录定义了这些操作的顺序
- 修改后，文件区域的状态取决于修改的类型、修改成功与否、以及是否存在并发修改？

### 文件的状态

- 无论从哪个副本读取数据，如果所有`Client`获得的数据相同，那么就称他为`consistent`
- 如果一个区域在修改后是`一致的`，而且`Client`可以看到修改后的内容，那么就称他为`deﬁned`
- 当一个修改完成，且没有并发写入的干扰，那么这个区域就是`defined`（也是`consistent`）的：所有`Client`都能看到修改所写入的内容
- 并发写入成功，区域会处于`undefine`但是`consistent`的状态，所有`Client`都会得到相同的数据，但是数据不会完整反映任何一个修改所带来的内容。通常来说，这个写入由多个修改综合得来。
- 修改失败会导致区域`不一致`（同时也`未定义`）：此时不同的客户端可能在不同的时间看到不同的数据

### 如何保证数据副本一致性

- 数据修改包含两种操作：写入和追加
- 写入是在指定的偏移处写入，
- 追加：即便在并发写入的状况下，数据仍将至少被原子性的追加一次，但是是在GFS选择的偏移量后追加，而不是通常以为的文件末尾追加，偏移量会返回给客户端
- 经过一连串成功的修改后，修改后的区域可以保证是`定义的`，并且包含了最后一次修改的数据，GFS通过以下两个措施保证
  1. 在所有数据副本上以相同的顺序对一个块应用修改(3.1节)
  2. 使用块版本号检测副本是否因为Down机而过时，过期的副本不会执行修改，也不会在Client请求时，被master提供出去，而是会被GC收集处理

### 解决过时Cache

- 由于客户端缓存了块的位置，可能会在刷新之前从过时的副本中读取数据。
- 这个窗口受到缓存项超时和文件下次打开的共同限制，该文件将从缓存中清除该文件的所有`chunk`信息。
- 由于大多数文件是只追加的，访问过期副本通常得到的是块提前结束，而不是过期的数据。
- 当一个`Client`尝试联系master时，它会得到最新块的位置

### 所有chunk失效

- 在成功修改后的一段时间，组件的故障仍然可能会损坏或破坏数据。
- GFS Master通过握手来识别失败的块服务器，并通过校验和来检测数据损坏(章节5.2)。一旦出现问题，数据就会从有效的副本中尽快恢复(第4.3节)。
- 只有当所有副本在GFS做出反应之前同时丢失时，一个数据块才会不可逆转地丢失，这通常在几分钟内发生。在这种情况下，它会变的不可用，而不是损坏，应用程序收到的是错误，而不是损坏的数据。

## 2.对应用程序的影响

使用GFS的应用可以通过以下技术来适应宽松的一致性模型：

- 依靠追加而不是覆盖，检查点
- 编写自我验证、自我识别的记录

实际使用中，我们使用追加而不是重写来修改文件。

比如，`Writer`从头到尾写入内容，生成一个文件。写完后，自动将文件重命名为一个永久的名称，定期检查成功写入的数据量。读取器只往前验证到最后一个检查点的文件区域，已知该区域处于`define`状态。不考虑一致性和并发问题。与随机写入相比，追加的效率更高，故障恢复能力更强。 检查点允许写入者以增量的方式重新启动，并使读取者不要处理已成功写入的区域。

在案例中，多个Writer同时对一个文件进行追加，以获得合并的结果。 记录追加的append-at-least-once语义保留了每个Writer的输入。 Writer写入的每条记录都包含额外的信息，如校验和，可以验证有效性。 读取器可以通过这些信息识别并丢弃重复的片段。 如果它不能容忍偶尔的重复（例如，如果它们会触发非幂等操作），它可以使用记录中的唯一标识符将它们过滤掉，这些标识符通常需要用来命名相应的应用实体，如网络文件。 这些记录I/O的功能（除了重复的删除）都在我们的应用程序共享的库代码中，适用于谷歌的其他文件接口实现。 这样一来，相同的记录序列，加上极少出现的重复记录，会被传递给reader。

# 4.MASTER OPERATION

## 4.1 Namespace Management and Locking

Master操作需要很长的时间：例如，进行一个快照操作必须撤销快照所覆盖的所有块上的chunkserver租约。同事，我们不希望暂停其他Master操作。我们允许多个操作处于活动状态，对命名空间的区域使用锁以确保正确的操作序列。

与许多传统的文件系统不同，GFS没有一个按目录的数据结构来列出该目录包含的所有文件。它也不支持文件或目录的别名功能（即Unix术语中的硬链接或符号链接）。GFS将命名空间表示为一个将路径名映射到元数据的查询表。通过编码压缩，该表可以整体放在内存中。命名空间树上的每个节点（文件或目录）都有一个读写锁。

每个操作在运行前都会获得一组锁。如果它涉及`/d1/d2/.../dn/leaf`，它将获得目录`/d1、/d1/d2、...、/d1/d2/.../dn`的读锁，以及`/d1/d2/.../dn/leaf`的读或写锁。注意，`leaf`可以是一个文件或目录，这取决于操作。

下面简单说明，当快照`/home/user`到`/save/user`时，锁定机制如何防止创建一个`/home/user/foo`文件。

快照操作获得了`/home`和`/save`的读锁，`/home/user`和`/save/user`的写锁。

创建文件获得了`/home`和`/home/user`的读锁，以及`/home/user/foo`的写锁。

这两个操作会被正确地排序，因为它们同时试图获得`/home/user`上的锁。创建文件不需要父目录的写锁，因为没有"目录"，或类似inode的数据结构需要保护，以免被修改。名称上的读锁能够保证父目录不被删除。

这种锁定方案的一个好处是，它允许在同一目录下同时进行修改。例如，可以在同一个目录中同时创建多个文件：每个文件都获得了对目录名的读锁和对文件名的写锁。目录名上的读锁可以防止目录被删除、重命名或快照。文件名上的写锁可以将创建同名文件的尝试序列化。

由于命名空间可以有很多节点，读写锁对象被懒散地分配，一旦不使用就被删除。此外，为防止死锁，锁的获取顺序是一致的：首先按命名空间树的级别排序，然后在同一级别内按词法排序。 

## 4.2 Replica Placement

GFS集群

- 通常有数百个chunkservers，分布在许多机房。
- 这些chunkservers被来自相同或不同机房的数百个客户端访问。机器之间的通信可能会跨越一个或多个交换机。
- 向外发送时的带宽大概率小于机房内部的带宽。多级分布对分布数据的可扩展性、可靠性和可用性提出了独特的挑战。

Chunk 复制品的放置策略有两个目的：

1. 最大化数据的可靠性和可用性
2. 最大化网络带宽的利用率。

## 4.3 Creation, Re-replication, Rebalancing
